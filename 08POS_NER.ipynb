{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNB2VsJE9mUippfD43TBCBm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!python -m spacy download en_core_web_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rg_BO9q-YO03","executionInfo":{"status":"ok","timestamp":1723881689343,"user_tz":-120,"elapsed":17843,"user":{"displayName":"ivan Montejo","userId":"10975568798522034191"}},"outputId":"8ea33762-ca2d-4b5c-9e49-28bae4d0cd2e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm==3.7.1\n","  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","import pandas as pd\n","import re\n","import nltk.corpus as corpus\n","import nltk\n","import spacy\n","import math\n","from warnings import simplefilter\n","simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n","#import csv\n","nltk.download(\"brown\")\n","nltk.download(\"treebank\")\n","nltk.download('universal_tagset')\n","nltk.download('treebank_tagset')\n","pd.options.mode.chained_assignment = None  # default='warn'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eWShy51cYSHg","executionInfo":{"status":"ok","timestamp":1723881700504,"user_tz":-120,"elapsed":11177,"user":{"displayName":"ivan Montejo","userId":"10975568798522034191"}},"outputId":"460adb8d-bc7f-4bc6-8b41-15f384be6b20"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","[nltk_data] Error loading treebank_tagset: Package 'treebank_tagset'\n","[nltk_data]     not found in index\n"]}]},{"cell_type":"code","source":["def removePunctuation(s):\n","  out = re.sub(\"[^\\w\\s\\u0300-\\u036f]\", \"\", s)\n","  out = re.sub(\"\\d+\", \" \", out)\n","  out = re.sub(\":\", \"\", out)\n","  out = re.sub(\"[\\\\\\n]\", \" \", out)\n","  out = re.sub(\" +\", \" \", out)\n","  return out"],"metadata":{"id":"m0Q-k46eYasV","executionInfo":{"status":"ok","timestamp":1723881700504,"user_tz":-120,"elapsed":17,"user":{"displayName":"ivan Montejo","userId":"10975568798522034191"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")\n","corpus_treebank = corpus.treebank.tagged_sents()\n","corpus_brown = corpus.treebank.tagged_sents()\n","tagged_sents_full = corpus_brown + corpus_treebank"],"metadata":{"id":"t4MTI1mbYs_w","executionInfo":{"status":"ok","timestamp":1723881703167,"user_tz":-120,"elapsed":2675,"user":{"displayName":"ivan Montejo","userId":"10975568798522034191"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["lim = 10000\n","tagged_sents = tagged_sents_full[:lim]\n","tagged_sents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olWLfxSwe9pN","executionInfo":{"status":"ok","timestamp":1723881708400,"user_tz":-120,"elapsed":5242,"user":{"displayName":"ivan Montejo","userId":"10975568798522034191"}},"outputId":"7479b24f-377a-4d88-ea62-6f3003b04859"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# Parts of speech and Named entities\n","\n","Parts of speech refers roughly to the morphosyntactic class a word belongs to and it's given to each word individually or, alternatively in some languages, to morphemes. Named entities are, roughly, anything that can be referred to with a proper name such as a person, a location or an organization.\n","\n","POS and named entities are useful clues for sentence structure and meaning and POS tagging is a key aspect of parsing. In a sequence of words each individual word gets assigned a tag following its word type (Noun, verb, adj...) and other aspects such as inflection. The task of named entity recognition is to assign words of phrases tags like \"ORGANIZATION\", \"PERSON\" or \"LOCATION\".\n","\n","The task of given each word a tag in a sequence is called sequence labeling, some examples are the hidden Markov model (HMM, generative) and the Conditional Random Field (CRM, discriminative). There are other more advanced methods like RNN."],"metadata":{"id":"Yf9u80o5LLAY"}},{"cell_type":"markdown","source":["## Parts of speech\n","\n","Parts of speech are defined based on their grammatical relationship with its neighboring words or the morphological properties about their affixes or inflections. There are a los of word distinctions like open and closed classes, functional and conceptual, etc. The most extended tags are the Penn Treebank POS tags."],"metadata":{"id":"MrZ6_DDDyyUe"}},{"cell_type":"markdown","source":["### Tagging\n"," Tagging is a process os disambiguation, as words are ambiguous. There are, generally, highly accurate POS tagging algorithms. Ambiguous words are very common, they represent 14-15% of the vocabulary but they are very common, 55-67%. The idea then is to choose the tag which is most frequent in the training corpus given an ambiguous word."],"metadata":{"id":"oiJC4OA4yyKW"}},{"cell_type":"markdown","source":["## Named entities\n","\n"," NAmed entities are anything that can be refered to with a proper name. The task or named entity recognition is to find spans of text that constitute proper names and tag the type of entity. The four most common tags are: PER (person), LOC (location), ORG (organization) and GPE (Geo-political entity). Many applications will need to use specific entity types.\n","\n"," NER is an important step in NLP, as, for example, in sentiment analysis we might want to know the consumer's sentiment toward a particular entity.\n","\n"," Named entities have an ambiguity of segmentation, meaning it's hard to decide what's part of the entity and what not, or where it starts and ends. The standard approach to NER is BIO tagging, this allows us to treat the problem as a word by word labeling task by labeling the boundary and the named entity type. There are other methods like the IO (I for named entities (inside), O for the rest) or the BIOES (E for end and S for span of one word)."],"metadata":{"id":"ddPCS9-Vyx4i"}},{"cell_type":"markdown","source":["## Hidden Markov POS Model Tagging\n","\n","It assigns a label to each unit in the sequence. It's a probabilistic model, it computes the probability distribution over possible sequences of labels and chooses the best label sequence."],"metadata":{"id":"2RSC-5rZyqXQ"}},{"cell_type":"markdown","source":["### Markov Chains\n","\n","A markov chain is a model that tells us something about the probabilities of the sequences of random variables. It makes the assumption that if we want to predict the future in the sequence all that matters is the current state.\n","\n","    P(q_i = a | q_1, q_2...q_i-1) = P(p_i = a | q-1)\n","\n","So, it defines the probability of each change in state given a current state, all of which sum up to one in each state. This is similar to bigram models. It has the following components:\n","\n","* A set of N states: Q = q_1, q_2... q_n\n","* A transition probability matrix A: Each a_i_j represents the probability of moving from state *i* to state *j*.\n","* An initial probability matrix over states (pi): pi_i is the probability that the Markov chain will start in state *i*."],"metadata":{"id":"5idicIDvys5P"}},{"cell_type":"markdown","source":["### Hidden Markov Model\n","\n","A MArkov chain is useful to compute a probability for a sequence of observable events, but most of the times we are interested in the hidden events. These events are hidden because we can't observe them directly, we must infer them. The hidden markov model has the following components:\n","\n","* A set of N states: Q = q_1, q_2... q_n\n","* A transition probability matrix A: Each a_i_j represents the probability of moving from state *i* to state *j*.\n","* A sequence T of observations: O = o_1, o_2... o_T each one drawn from a vocabulary V = v_1, v_2... v_v\n","* A sequence of observation likelihoods: B = b_i(o_i), also called emission probability, each representing the probability of an observation o_i being generated from a state q.\n","* An initial probability matrix over states (pi): pi_i is the probability that the Markov chain will start in state *i*.\n","\n","The model assumes, in first place, that the probability of a particular state depends only on the previous state. In second place it assumes that the probability of an output observation o_i depends only on the state that produced the observation q_i and not on any other states or observations.\n","\n","The basic components of an HMM tagger are the two probabilities matrices A and B. The matrix A contains the tag transition probabilities, meaning the probabilities of a tag ocurring given a previous tag. This is computed though maximum likelihood estimate (MLE):\n","\n","    P(t_i | t_i-1) = count(t_i-1, t_i)/count(t_i-1)\n","\n","Remember it estimates the tag *t_i*, so the probability is based on the previous tag, taht's why the denominator is the previous tag.\n","\n","The matrix B is composed of the emission probabilities and represents the probability, given a tag, that it'll be associated with a given word, also computed through MLE:\n","\n","    P(w_i | t_i) = count(t_i, w_i)/count(t_i)\n","\n","The task of determining the hidden variables sequence corresponding to the sequence of observations is called decoding. It's to find the most probable sequence of states (Q = q_i, q_2... q_n) given the two matrices A and B and a sequence of observations (O = o_q, o_2... o_n), which is also the observation sequence of n words w_1, w_2... w_n.\n","\n","The two assumptions are:\n","\n","* The probability of a word appearing depends only on its own tag.\n","* The probability of a tag is dependen only on the previous tag."],"metadata":{"id":"F-gZ4ndbyk3s"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"U8jJkK5CKqsV","executionInfo":{"status":"ok","timestamp":1723882052649,"user_tz":-120,"elapsed":19462,"user":{"displayName":"ivan Montejo","userId":"10975568798522034191"}}},"outputs":[],"source":["# The two matrices A and B\n","try:\n","  trans_prob = pd.read_excel(\"/content/ProbabilityMatrix.xlsx\", sheet_name=\"Transmission Probabilities\", index_col=0, engine=\"openpyxl\")\n","  emission_prob = pd.read_excel(\"/content/ProbabilityMatrix.xlsx\", sheet_name=\"Emission Probabilities\", index_col=0, engine=\"openpyxl\")\n","except:\n","  trans_prob = pd.DataFrame(index=[\"<s>\"])\n","  emission_prob = pd.DataFrame()\n","\n","  for i in tagged_sents:\n","    for j in range(len(i)):\n","      if i[j][0] not in emission_prob.columns:\n","        emission_prob[i[j][0]] = 0\n","      if i[j][1] not in emission_prob.index:\n","        emission_prob.loc[i[j][1]] = 0\n","      emission_prob[i[j][0]][i[j][1]] += 1\n","\n","      if i[j][1] not in trans_prob.columns:\n","        trans_prob[i[j][1]] = 0\n","        trans_prob.loc[i[j][1]] = 0\n","\n","      if j != 0:\n","        trans_prob[i[j][1]][i[j-1][1]] += 1\n","      else:\n","        trans_prob[i[j][1]][\"<s>\"] += 1\n","\n","  trans_prob = trans_prob.div([sum(trans_prob.loc[i]) for i in trans_prob.index], axis=\"index\")\n","  emission_prob = emission_prob.div([sum(emission_prob.loc[i]) for i in emission_prob.index], axis=\"index\")\n","  writer = pd.ExcelWriter(\"/content/out.xlsx\", engine = \"openpyxl\")\n","  trans_prob.to_excel(writer, sheet_name=\"Transmission Probabilities\")\n","  emission_prob.to_excel(writer, sheet_name=\"Emission Probabilities\")\n","  writer.close()"]},{"cell_type":"code","source":["def getMaxHMM(w, prev_tag):\n","  out = \"\"\n","  currentMax = 0\n","\n","  if w in emission_prob.columns:\n","    for c in trans_prob.columns:\n","      p = (trans_prob[c][prev_tag]**0.5)*(emission_prob[w][c]**0.5)\n","      if p > currentMax:\n","        out = c\n","        currentMax = p\n","  else:\n","    out = trans_prob.loc[prev_tag].idxmax()\n","\n","  if currentMax <= 0:\n","    out = trans_prob.loc[prev_tag].idxmax()\n","\n","  return out\n","\n","def markov(s):\n","  words = [\"<s>\"] + [i.text for i in nlp(s)]\n","  out = [(words[0], words[0])]\n","  for i in range(1, len(words)):\n","    out.append((words[i], getMaxHMM(words[i], out[i-1][1])))\n","  return out"],"metadata":{"id":"ViPOk0fOgL78","executionInfo":{"status":"ok","timestamp":1723885547835,"user_tz":-120,"elapsed":335,"user":{"displayName":"ivan Montejo","userId":"10975568798522034191"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["test_sentence = \"more than half of the journeys taken from London City airport last year can be reached in six hours or less by train, data reveals.\"\n","print(markov(test_sentence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88Gs7BYJgAD6","executionInfo":{"status":"ok","timestamp":1723885597599,"user_tz":-120,"elapsed":313,"user":{"displayName":"ivan Montejo","userId":"10975568798522034191"}},"outputId":"8515d2c8-a752-46c6-cba9-029f5228a168"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["[('<s>', '<s>'), ('more', 'JJR'), ('than', 'IN'), ('half', 'DT'), ('of', 'IN'), ('the', 'DT'), ('journeys', 'NN'), ('taken', 'VBN'), ('from', 'IN'), ('London', 'NNP'), ('City', 'NNP'), ('airport', 'NNP'), ('last', 'JJ'), ('year', 'NN'), ('can', 'MD'), ('be', 'VB'), ('reached', 'VBN'), ('in', 'IN'), ('six', 'CD'), ('hours', 'NNS'), ('or', 'CC'), ('less', 'JJR'), ('by', 'IN'), ('train', 'NN'), (',', ','), ('data', 'NNS'), ('reveals', 'IN'), ('.', '.')]\n"]}]},{"cell_type":"markdown","source":["## Conditional Random Fields (CRFs)\n","\n","This model is better at tagging words that are not in the vocabulary. This sistem is based in morphological or capitalization rules. This model also takes into account previous or following words. In general this model combines arbitrary features in a principled way, just like log-linear models. The problem is that logistic regression assigns a class to a single observation, but we need a sequential model.\n","\n","The most commonly used versio of the CRF in NLP is the Linear chain CRF, whose conditioning closely matches the HMM. Assuming we have a sequence of input words X = x_1, x_2... x_n and want to compute the sequence of output tags Y = y_1, y_2... y_n in HMM we rely on bayes rule and the likelihood. In CRF we compute the posterior p(Y | X) directly, training the CRF to discriminate among the tags. At each timestep the CRF computes the log-linear functions over a set of relevant features, and these local features are aggregated and normalized to create a global probability for the whole sequence.\n","\n","A CRF assigns a probability to an entire output (tags) sequence out of al possible sequences, given the entire input (words). In regular logistic regression the feature function *f* computes the features of a tuple (x, y). In CRF the function *F* maps an entire input sequence *X* and an entire output sequence *Y* to a feature vector"],"metadata":{"id":"L2PbLTBIyJB2"}}]}